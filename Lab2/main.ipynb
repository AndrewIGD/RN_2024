{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load MNIST dataset",
   "id": "9795ce7fa8efc9c5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T08:35:18.432627Z",
     "start_time": "2024-10-21T08:35:17.847369Z"
    }
   },
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root=\"./data\",\n",
    "                    transform=lambda x: np.array(x).flatten(),\n",
    "                    download=True,\n",
    "                    train=is_train)\n",
    "\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "\n",
    "    return mnist_data, mnist_labels\n",
    "\n",
    "train_data, train_labels = download_mnist(True)\n",
    "test_data, test_labels = download_mnist(False)\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalize data and Convert labels",
   "id": "478ccbdf7efcd94c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T08:35:23.774027Z",
     "start_time": "2024-10-21T08:35:23.700804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize(data, min_value, max_value):\n",
    "    return (data - min_value) / (max_value - min_value)\n",
    "\n",
    "def one_hot_encode(labels, num_categories):\n",
    "    length = len(labels)\n",
    "    converted_labels = np.full((length, num_categories), 0, dtype=np.int32)\n",
    "\n",
    "    for i in range(length):\n",
    "        converted_labels[i][labels[i]] = 1\n",
    "        \n",
    "    return converted_labels\n",
    "\n",
    "classes = np.max(train_labels) + 1\n",
    "\n",
    "min_value = min(np.min(train_data), np.min(test_data))\n",
    "max_value = max(np.max(train_data), np.max(test_data))\n",
    "\n",
    "train_data = normalize(train_data, min_value, max_value)\n",
    "test_data = normalize(test_data, min_value, max_value)\n",
    "\n",
    "train_labels = one_hot_encode(train_labels, 10)\n",
    "test_labels = one_hot_encode(test_labels, 10)"
   ],
   "id": "9e03b6d81fcb418f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(classes)",
   "id": "784d38da5688c393",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "15a046668518427e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T11:28:04.037386Z",
     "start_time": "2024-10-21T11:26:00.181083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 100\n",
    "learning_factor = 0.01\n",
    "inputs = len(train_data[0])\n",
    "weights = np.zeros((classes, inputs), dtype=np.float32)\n",
    "bias = np.zeros(classes, dtype=np.float32)\n",
    "\n",
    "for i in range(classes):\n",
    "    for j in range(inputs):\n",
    "        weights[i][j] = random.uniform(0.01, 0.99)\n",
    "        \n",
    "for i in range(classes):\n",
    "    bias[i] = random.uniform(0.01, 0.99)\n",
    "\n",
    "def split_batch(data, labels):\n",
    "    num_batches = len(data) // batch_size + 1\n",
    "\n",
    "    data_batches = []\n",
    "    label_batches = []\n",
    "\n",
    "    for i in range(num_batches - 1):\n",
    "        data_batches.append(data[i * batch_size:(i + 1) * batch_size])\n",
    "        label_batches.append(labels[i * batch_size:(i + 1) * batch_size])\n",
    "\n",
    "    data_batches.append(data[(num_batches - 1) * batch_size:])\n",
    "    label_batches.append(labels[(num_batches - 1) * batch_size:])\n",
    "\n",
    "    return data_batches, label_batches\n",
    "\n",
    "\n",
    "def forward_propagation(sample_data, sample_labels):\n",
    "    class_sums = np.zeros(classes, dtype=np.float32)\n",
    "\n",
    "    for _class in range(classes):\n",
    "        class_sums[_class] = np.dot(sample_data, weights[_class]) + bias[_class]\n",
    "        \n",
    "    max_class_sum = np.max(class_sums)\n",
    "    class_sums -= max_class_sum\n",
    "    \n",
    "    exp_class_sums = np.exp(class_sums)\n",
    "    \n",
    "    probabilities = exp_class_sums / np.sum(exp_class_sums)\n",
    "\n",
    "    predicted_class = np.argmax(probabilities)\n",
    "\n",
    "    error = sample_labels - probabilities\n",
    "\n",
    "    return predicted_class, error\n",
    "\n",
    "\n",
    "def train(data_batch, label_batch, weights, bias):\n",
    "    for batch_index in range(len(data_batch)):\n",
    "        sample_data = data_batch[batch_index]\n",
    "        sample_labels = label_batch[batch_index]\n",
    "\n",
    "        predicted_class, error = forward_propagation(sample_data, sample_labels)\n",
    "\n",
    "        for _class in range(classes):\n",
    "            weights[_class] += learning_factor * error[_class] * sample_data\n",
    "            bias[_class] += learning_factor * error[_class]\n",
    "            \n",
    "    return weights, bias\n",
    "\n",
    "def get_perceptron_accuracy():\n",
    "    length = len(test_data)\n",
    "    success_cases = 0\n",
    "    for i in range(length):\n",
    "        predicted_class, error = forward_propagation(test_data[i], test_labels[i])\n",
    "        \n",
    "        if test_labels[i][predicted_class] == 1:\n",
    "            success_cases += 1\n",
    "            \n",
    "    return success_cases / length\n",
    "\n",
    "data_batches, label_batches = split_batch(train_data, train_labels)\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Processing: \" + str(epoch) + \"/\" + str(num_epochs) + \": \" + str(get_perceptron_accuracy()))\n",
    "    for i in range(len(data_batches)):\n",
    "        new_weights, new_biases = train(data_batches[i], label_batches[i], weights, bias)\n",
    "        weights = new_weights\n",
    "        bias = new_biases"
   ],
   "id": "99bb7e1970225c38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0/50: 0.0697\n",
      "Processing: 1/50: 0.9001\n",
      "Processing: 2/50: 0.9063\n",
      "Processing: 3/50: 0.9085\n",
      "Processing: 4/50: 0.9098\n",
      "Processing: 5/50: 0.9111\n",
      "Processing: 6/50: 0.9118\n",
      "Processing: 7/50: 0.9117\n",
      "Processing: 8/50: 0.9121\n",
      "Processing: 9/50: 0.9121\n",
      "Processing: 10/50: 0.9122\n",
      "Processing: 11/50: 0.9122\n",
      "Processing: 12/50: 0.912\n",
      "Processing: 13/50: 0.9119\n",
      "Processing: 14/50: 0.9122\n",
      "Processing: 15/50: 0.9123\n",
      "Processing: 16/50: 0.912\n",
      "Processing: 17/50: 0.912\n",
      "Processing: 18/50: 0.9122\n",
      "Processing: 19/50: 0.9131\n",
      "Processing: 20/50: 0.9133\n",
      "Processing: 21/50: 0.9132\n",
      "Processing: 22/50: 0.913\n",
      "Processing: 23/50: 0.913\n",
      "Processing: 24/50: 0.913\n",
      "Processing: 25/50: 0.9132\n",
      "Processing: 26/50: 0.9131\n",
      "Processing: 27/50: 0.9132\n",
      "Processing: 28/50: 0.9134\n",
      "Processing: 29/50: 0.9133\n",
      "Processing: 30/50: 0.9137\n",
      "Processing: 31/50: 0.9138\n",
      "Processing: 32/50: 0.914\n",
      "Processing: 33/50: 0.9141\n",
      "Processing: 34/50: 0.914\n",
      "Processing: 35/50: 0.9141\n",
      "Processing: 36/50: 0.9141\n",
      "Processing: 37/50: 0.9144\n",
      "Processing: 38/50: 0.9145\n",
      "Processing: 39/50: 0.9145\n",
      "Processing: 40/50: 0.9141\n",
      "Processing: 41/50: 0.9141\n",
      "Processing: 42/50: 0.9141\n",
      "Processing: 43/50: 0.914\n",
      "Processing: 44/50: 0.9139\n",
      "Processing: 45/50: 0.914\n",
      "Processing: 46/50: 0.9141\n",
      "Processing: 47/50: 0.9143\n",
      "Processing: 48/50: 0.9144\n",
      "Processing: 49/50: 0.9144\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing",
   "id": "b600ed03cc06bd27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T11:28:06.683198Z",
     "start_time": "2024-10-21T11:28:06.507971Z"
    }
   },
   "cell_type": "code",
   "source": "print(get_perceptron_accuracy())",
   "id": "b7d638bb2000671a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9143\n"
     ]
    }
   ],
   "execution_count": 100
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
