{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-28T12:45:51.678174Z",
     "start_time": "2024-10-28T12:45:51.675350Z"
    }
   },
   "source": [
    "# 3. Utilize batched operations for enhanced operations\n",
    "# Asta inseamna sa folosesti mini-batch-uri si sa folosesti\n",
    "# pe cat se poate operatii de numpy, adica spre ex: sa nu\n",
    "# faci inmultirile de mana cand poti face dot product\n",
    "\n",
    "# Training-ul ar trebui sa ia cam 5-6min\n",
    "\n",
    "# As zice ca implementarea dropout suna cel mai bine\n",
    "# E posibil ca unii neuroni sa puna monopol pe output, vedem cum se\n",
    "# descurca reteaua fara ei si dam update la retea"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize MNIST dataset",
   "id": "f033671043324697"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T05:59:35.864665Z",
     "start_time": "2024-11-11T05:59:33.753564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root=\"./data\",\n",
    "                    transform=lambda x: np.array(x).flatten(),\n",
    "                    download=True,\n",
    "                    train=is_train)\n",
    "\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "\n",
    "    return mnist_data, mnist_labels\n",
    "\n",
    "train_data, train_labels = download_mnist(True)\n",
    "test_data, test_labels = download_mnist(False)\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)"
   ],
   "id": "71aa67c1191d0703",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalize data and Convert labels to One-Hot Encoding",
   "id": "4ceae6234b079f46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T05:59:37.799293Z",
     "start_time": "2024-11-11T05:59:37.728122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize(data, min_value, max_value):\n",
    "    return (data - min_value) / (max_value - min_value)\n",
    "\n",
    "def one_hot_encode(labels, num_categories):\n",
    "    length = len(labels)\n",
    "    converted_labels = np.full((length, num_categories), 0, dtype=np.int32)\n",
    "\n",
    "    for i in range(length):\n",
    "        converted_labels[i][labels[i]] = 1\n",
    "\n",
    "    return converted_labels\n",
    "\n",
    "classes = np.max(train_labels) + 1\n",
    "\n",
    "min_value = min(np.min(train_data), np.min(test_data))\n",
    "max_value = max(np.max(train_data), np.max(test_data))\n",
    "\n",
    "train_data = normalize(train_data, min_value, max_value)\n",
    "test_data = normalize(test_data, min_value, max_value)\n",
    "\n",
    "train_labels = one_hot_encode(train_labels, 10)\n",
    "test_labels = one_hot_encode(test_labels, 10)\n",
    "\n",
    "num_inputs = len(train_data[0])"
   ],
   "id": "2f9b877950e6333f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NN Training",
   "id": "7684ea7dc7dd9482"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T07:44:51.152090Z",
     "start_time": "2024-11-11T07:41:47.148238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "hidden_layers = 3\n",
    "hidden_neurons_per_layer = 16\n",
    "num_epochs = 1000\n",
    "batch_size = 128\n",
    "learning_rate = 0.002\n",
    "slow_learning_rate = 0.0005\n",
    "dropout_neurons = 4\n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "\n",
    "tester = None\n",
    "cost_function = None\n",
    "\n",
    "class Function:\n",
    "    def apply(self, *values):\n",
    "        pass\n",
    "    \n",
    "    def apply_derivative(self, *values):\n",
    "        pass\n",
    "    \n",
    "class Relu(Function):\n",
    "    def apply(self, *values):\n",
    "        if values[0] > 0:\n",
    "            return values[0]\n",
    "        \n",
    "        return 0.01 * values[0]\n",
    "    \n",
    "    def apply_derivative(self, *values):\n",
    "        if values[0] > 0:\n",
    "            return 1\n",
    "        \n",
    "        return 0.01\n",
    "    \n",
    "class Softmax(Function):\n",
    "    def __init__(self, values):\n",
    "        self.base_values = values.copy()\n",
    "        self.values = np.exp(values)\n",
    "        self.values /= np.max(self.values)\n",
    "        \n",
    "        self.cached_sum = np.sum(self.values)\n",
    "    \n",
    "    def apply(self, *values): \n",
    "        index = values[0]\n",
    "        \n",
    "        return self.values[index] / self.cached_sum\n",
    "    \n",
    "    def apply_derivative(self, *values):\n",
    "        index = values[0]\n",
    "        \n",
    "        if index == len(self.base_values) - 1:\n",
    "            return self.base_values[index] * (1 - self.base_values[index])\n",
    "        \n",
    "        return -self.base_values[index] * self.base_values[-1]\n",
    "    \n",
    "class CrossEntropy(Function):\n",
    "    def apply(self, *values):\n",
    "        value = values[0]\n",
    "        target = values[1]\n",
    "        \n",
    "        return value - target\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, num_inputs, neuron_index, activation_function):\n",
    "        std = math.sqrt(2.0 / num_inputs) # He init\n",
    "        self.weights = np.random.randn(num_inputs) * std\n",
    "        self.bias = np.random.randn() * std\n",
    "        self.num_inputs = num_inputs\n",
    "        self.neuron_index = neuron_index\n",
    "        self.value = 0\n",
    "        self.net_sum = 0\n",
    "        self.error = 0\n",
    "        self.activation_function = activation_function\n",
    "        self.dropout = False\n",
    "        self.weight_momentum = np.zeros(num_inputs)\n",
    "        self.bias_momentum = 0\n",
    "        \n",
    "    def set_dropout(self, value):\n",
    "        self.dropout = value\n",
    "    \n",
    "    def compute(self, data):\n",
    "        if self.dropout:\n",
    "            self.net_sum = 0\n",
    "            self.value = 0\n",
    "            return\n",
    "        \n",
    "        self.net_sum = np.dot(self.weights, data) + self.bias\n",
    "        self.value = self.activation_function.apply(self.net_sum)\n",
    "    \n",
    "    def compute_error(self, influenced_neurons):\n",
    "        self.error = 0\n",
    "        for i in range(len(influenced_neurons)):\n",
    "            self.error += influenced_neurons[i].error * influenced_neurons[i].weights[self.neuron_index]\n",
    "        self.error *= self.activation_function.apply_derivative(self.net_sum)\n",
    "            \n",
    "    def adjust_weights(self, influencer_neurons):\n",
    "        for i in range(len(self.weights)):\n",
    "            #self.weight_momentum[i] = self.weight_momentum[i] * momentum + (self.error * influencer_neurons[i].value + weight_decay * self.weights[i])\n",
    "            #self.weights[i] -= learning_rate * self.weight_momentum[i]\n",
    "    \n",
    "            self.weights[i] -= learning_rate * (self.error * influencer_neurons[i].value + weight_decay * self.weights[i])\n",
    "            \n",
    "        #self.bias_momentum = self.bias_momentum * momentum * self.error\n",
    "        #self.bias -= learning_rate * self.bias_momentum\n",
    "        \n",
    "        self.bias -= learning_rate * self.error\n",
    "    \n",
    "class InputNeuron(Neuron):\n",
    "    def compute(self, data):\n",
    "        self.value = data[self.neuron_index] \n",
    "\n",
    "class OutputNeuron(Neuron):\n",
    "    def __init__(self, num_inputs, neuron_index, activation_function):\n",
    "        super().__init__(num_inputs, neuron_index, activation_function)\n",
    "        self.target = 0\n",
    "        \n",
    "    def compute(self, data):\n",
    "        self.net_sum = np.dot(self.weights, data) + self.bias\n",
    "        \n",
    "    def compute_probability(self):\n",
    "        self.value = self.activation_function.apply(self.neuron_index)\n",
    "        \n",
    "    def compute_error(self, target):\n",
    "        self.error = self.value - target\n",
    "        \n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, num_neurons, neuron_inputs, neuron_type):\n",
    "        self.activation_function = Relu()\n",
    "        self.neurons = [neuron_type(neuron_inputs, i, self.activation_function) for i in range(num_neurons)]\n",
    "        \n",
    "    def compute(self, data):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.compute(data)\n",
    "            \n",
    "    def get_values(self):\n",
    "        return np.array([neuron.value for neuron in self.neurons])\n",
    "    \n",
    "    def get_net_sums(self):\n",
    "        return np.array([neuron.net_sum for neuron in self.neurons])\n",
    "    \n",
    "    def compute_error(self, next_layer):\n",
    "        next_layer_neurons = next_layer.neurons\n",
    "        for neuron in self.neurons:\n",
    "            neuron.compute_error(next_layer_neurons)\n",
    "            \n",
    "    def adjust_weights(self, prev_layer):\n",
    "        prev_layer_neurons = prev_layer.neurons\n",
    "        for neuron in self.neurons:\n",
    "            neuron.adjust_weights(prev_layer_neurons)\n",
    "    \n",
    "class OutputLayer(Layer):\n",
    "    def __init__(self, num_neurons, neuron_inputs):\n",
    "        super().__init__(num_neurons, neuron_inputs, OutputNeuron)\n",
    "        \n",
    "    def compute(self, data):\n",
    "        super().compute(data)\n",
    "        \n",
    "        self.activation_function = Softmax(self.get_net_sums())\n",
    "        \n",
    "        for neuron in self.neurons:\n",
    "            neuron.activation_function = self.activation_function\n",
    "            neuron.compute_probability()\n",
    "            \n",
    "    def compute_error(self, targets):\n",
    "        for i in range(len(self.neurons)):\n",
    "            self.neurons[i].compute_error(targets[i])\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.layers.append(Layer(num_inputs, 1, InputNeuron))\n",
    "        for i in range(hidden_layers):\n",
    "            self.layers.append(Layer(hidden_neurons_per_layer, len(self.layers[i].neurons), Neuron))\n",
    "            \n",
    "        self.layers.append(OutputLayer(classes, len(self.layers[-1].neurons)))\n",
    "        \n",
    "    def forward_pass(self, data):\n",
    "        self.layers[0].compute(data)\n",
    "        \n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].compute(self.layers[i-1].get_values())\n",
    "            \n",
    "        return self.layers[-1].get_values()\n",
    "    \n",
    "    def backprop(self, target):\n",
    "        self.layers[-1].compute_error(target)\n",
    "        self.layers[-1].adjust_weights(self.layers[-2])\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            self.layers[i].compute_error(self.layers[i+1])\n",
    "            self.layers[i].adjust_weights(self.layers[i-1])\n",
    "\n",
    "class TrainingSample:\n",
    "    def __init__(self, data, output):\n",
    "        self.input = data\n",
    "        self.output = output\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, nn: NeuralNetwork):\n",
    "        self.neural_network = nn\n",
    "        self.training_samples = []\n",
    "        for i in range(len(train_data)):\n",
    "            self.training_samples.append(TrainingSample(train_data[i], train_labels[i]))\n",
    "    \n",
    "    def get_mini_batches(self):\n",
    "        random.shuffle(self.training_samples)\n",
    "        \n",
    "        num_batches = len(self.training_samples) // batch_size + 1\n",
    "    \n",
    "        batches = []\n",
    "    \n",
    "        for i in range(num_batches - 1):\n",
    "            batches.append(self.training_samples[i * batch_size:(i + 1) * batch_size])\n",
    "    \n",
    "        batches.append(self.training_samples[(num_batches - 1) * batch_size:])\n",
    "    \n",
    "        return batches\n",
    "    \n",
    "    def start(self):\n",
    "        for epoch in range(num_epochs):\n",
    "            batches = self.get_mini_batches()\n",
    "            \n",
    "            for batch_index in range(len(batches)):\n",
    "                training_sample = self.training_samples[batch_index]\n",
    "                \n",
    "                neurons = []\n",
    "                \n",
    "                for i in range(dropout_neurons):\n",
    "                    layer = random.randint(1, len(self.neural_network.layers) - 2)\n",
    "                    neuron = random.randint(0, len(self.neural_network.layers[layer].neurons) - 1)\n",
    "                    \n",
    "                    neurons.append(self.neural_network.layers[layer].neurons[neuron])\n",
    "                    \n",
    "                    neurons[i].set_dropout(True)\n",
    "        \n",
    "                self.neural_network.forward_pass(training_sample.input)\n",
    "                \n",
    "                for i in range(dropout_neurons):\n",
    "                    neurons[i].set_dropout(False)\n",
    "                \n",
    "                self.neural_network.backprop(training_sample.output)\n",
    "                \n",
    "            accuracy = tester.get_accuracy() * 100\n",
    "                \n",
    "            if accuracy > 88:\n",
    "                global learning_rate\n",
    "                learning_rate = slow_learning_rate\n",
    "                \n",
    "            print(\"Epoch \" + str(epoch + 1) + \"/\" + str(num_epochs) + \": \" + str(accuracy) + \"% - \" + str(time.time() - start_time) + \"s\")\n",
    "\n",
    "class Tester:\n",
    "    def __init__(self, nn: NeuralNetwork):\n",
    "        self.neural_network = nn\n",
    "        self.test_samples = []\n",
    "        for i in range(len(test_data)):\n",
    "            self.test_samples.append(TrainingSample(test_data[i], test_labels[i]))\n",
    "        \n",
    "    def get_accuracy(self):\n",
    "        length = len(self.test_samples)\n",
    "        success_cases = 0\n",
    "        for i in range(length):\n",
    "            outputs = self.neural_network.forward_pass(test_data[i])\n",
    "            \n",
    "            prediction = np.argmax(outputs)\n",
    "            \n",
    "            if test_labels[i][prediction] == 1:\n",
    "                success_cases += 1\n",
    "                \n",
    "        return success_cases / length\n",
    "\n",
    "inner_activation_function = Relu()\n",
    "neural_network = NeuralNetwork()\n",
    "cost_function = CrossEntropy()\n",
    "tester = Tester(neural_network)\n",
    "trainer = Trainer(neural_network)\n",
    "trainer.start()"
   ],
   "id": "d9aacfcb60daaba4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: 19.040000000000003% - 2.755295991897583s\n",
      "Epoch 2/1000: 24.03% - 5.3779778480529785s\n",
      "Epoch 3/1000: 33.48% - 7.94980525970459s\n",
      "Epoch 4/1000: 34.23% - 10.54573392868042s\n",
      "Epoch 5/1000: 39.290000000000006% - 13.118313074111938s\n",
      "Epoch 6/1000: 42.84% - 15.761070966720581s\n",
      "Epoch 7/1000: 57.74% - 18.337092876434326s\n",
      "Epoch 8/1000: 63.51% - 20.916375160217285s\n",
      "Epoch 9/1000: 62.51% - 23.49451208114624s\n",
      "Epoch 10/1000: 64.62% - 26.16215991973877s\n",
      "Epoch 11/1000: 70.82000000000001% - 29.067703247070312s\n",
      "Epoch 12/1000: 67.86% - 31.69726014137268s\n",
      "Epoch 13/1000: 70.62% - 34.291430950164795s\n",
      "Epoch 14/1000: 73.37% - 36.94242215156555s\n",
      "Epoch 15/1000: 73.91% - 39.54590916633606s\n",
      "Epoch 16/1000: 77.21000000000001% - 42.1254301071167s\n",
      "Epoch 17/1000: 78.22% - 44.70270919799805s\n",
      "Epoch 18/1000: 80.74% - 47.33926701545715s\n",
      "Epoch 19/1000: 78.4% - 49.926454067230225s\n",
      "Epoch 20/1000: 79.71000000000001% - 52.50599718093872s\n",
      "Epoch 21/1000: 80.81% - 55.09620785713196s\n",
      "Epoch 22/1000: 81.10000000000001% - 57.73798608779907s\n",
      "Epoch 23/1000: 82.3% - 60.31538701057434s\n",
      "Epoch 24/1000: 82.47% - 62.90348696708679s\n",
      "Epoch 25/1000: 82.61% - 65.53109622001648s\n",
      "Epoch 26/1000: 84.43% - 68.12022113800049s\n",
      "Epoch 27/1000: 83.13000000000001% - 70.70144391059875s\n",
      "Epoch 28/1000: 83.8% - 73.28656911849976s\n",
      "Epoch 29/1000: 83.77% - 75.93632292747498s\n",
      "Epoch 30/1000: 85.0% - 78.52450704574585s\n",
      "Epoch 31/1000: 85.27% - 81.10560894012451s\n",
      "Epoch 32/1000: 85.5% - 83.69800996780396s\n",
      "Epoch 33/1000: 86.29% - 86.3795018196106s\n",
      "Epoch 34/1000: 85.28999999999999% - 89.21830010414124s\n",
      "Epoch 35/1000: 84.89% - 91.85803198814392s\n",
      "Epoch 36/1000: 85.55% - 94.44570922851562s\n",
      "Epoch 37/1000: 86.9% - 97.0829541683197s\n",
      "Epoch 38/1000: 87.58% - 99.67103910446167s\n",
      "Epoch 39/1000: 86.61% - 102.25863194465637s\n",
      "Epoch 40/1000: 88.13% - 104.85325503349304s\n",
      "Epoch 41/1000: 88.14% - 107.50621294975281s\n",
      "Epoch 42/1000: 88.14999999999999% - 110.09987688064575s\n",
      "Epoch 43/1000: 88.16000000000001% - 112.6912579536438s\n",
      "Epoch 44/1000: 88.23% - 115.29656100273132s\n",
      "Epoch 45/1000: 88.14999999999999% - 117.94387698173523s\n",
      "Epoch 46/1000: 88.14% - 120.5374801158905s\n",
      "Epoch 47/1000: 88.11% - 123.12464213371277s\n",
      "Epoch 48/1000: 88.06% - 125.71849822998047s\n",
      "Epoch 49/1000: 88.22% - 128.35738611221313s\n",
      "Epoch 50/1000: 88.14999999999999% - 130.94216895103455s\n",
      "Epoch 51/1000: 88.1% - 133.5176341533661s\n",
      "Epoch 52/1000: 88.24% - 136.11926794052124s\n",
      "Epoch 53/1000: 88.25% - 138.76078915596008s\n",
      "Epoch 54/1000: 88.25% - 141.35006022453308s\n",
      "Epoch 55/1000: 88.25% - 143.94464492797852s\n",
      "Epoch 56/1000: 88.28% - 146.54140400886536s\n",
      "Epoch 57/1000: 88.33% - 149.18494701385498s\n",
      "Epoch 58/1000: 88.24% - 151.77462720870972s\n",
      "Epoch 59/1000: 88.29% - 154.3504331111908s\n",
      "Epoch 60/1000: 88.26% - 156.9368772506714s\n",
      "Epoch 61/1000: 88.25% - 159.5804259777069s\n",
      "Epoch 62/1000: 88.25% - 162.15689992904663s\n",
      "Epoch 63/1000: 88.33% - 164.74439787864685s\n",
      "Epoch 64/1000: 88.32% - 167.35824394226074s\n",
      "Epoch 65/1000: 88.35% - 169.98349809646606s\n",
      "Epoch 66/1000: 88.33% - 172.57368993759155s\n",
      "Epoch 67/1000: 88.3% - 175.15606307983398s\n",
      "Epoch 68/1000: 88.39% - 177.80787897109985s\n",
      "Epoch 69/1000: 88.41% - 180.40962481498718s\n",
      "Epoch 70/1000: 88.36% - 183.00956511497498s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 278\u001B[0m\n\u001B[1;32m    276\u001B[0m tester \u001B[38;5;241m=\u001B[39m Tester(neural_network)\n\u001B[1;32m    277\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(neural_network)\n\u001B[0;32m--> 278\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[44], line 243\u001B[0m, in \u001B[0;36mTrainer.start\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(dropout_neurons):\n\u001B[1;32m    241\u001B[0m         neurons[i]\u001B[38;5;241m.\u001B[39mset_dropout(\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m--> 243\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mneural_network\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackprop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_sample\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    245\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m tester\u001B[38;5;241m.\u001B[39mget_accuracy() \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m accuracy \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m88\u001B[39m:\n",
      "Cell \u001B[0;32mIn[44], line 193\u001B[0m, in \u001B[0;36mNeuralNetwork.backprop\u001B[0;34m(self, target)\u001B[0m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[i]\u001B[38;5;241m.\u001B[39mcompute_error(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m--> 193\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madjust_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[44], line 152\u001B[0m, in \u001B[0;36mLayer.adjust_weights\u001B[0;34m(self, prev_layer)\u001B[0m\n\u001B[1;32m    150\u001B[0m prev_layer_neurons \u001B[38;5;241m=\u001B[39m prev_layer\u001B[38;5;241m.\u001B[39mneurons\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m neuron \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneurons:\n\u001B[0;32m--> 152\u001B[0m     \u001B[43mneuron\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madjust_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprev_layer_neurons\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[44], line 103\u001B[0m, in \u001B[0;36mNeuron.adjust_weights\u001B[0;34m(self, influencer_neurons)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madjust_weights\u001B[39m(\u001B[38;5;28mself\u001B[39m, influencer_neurons):\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights)):\n\u001B[1;32m    100\u001B[0m         \u001B[38;5;66;03m#self.weight_momentum[i] = self.weight_momentum[i] * momentum + (self.error * influencer_neurons[i].value + weight_decay * self.weights[i])\u001B[39;00m\n\u001B[1;32m    101\u001B[0m         \u001B[38;5;66;03m#self.weights[i] -= learning_rate * self.weight_momentum[i]\u001B[39;00m\n\u001B[0;32m--> 103\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights[i] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror \u001B[38;5;241m*\u001B[39m influencer_neurons[i]\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;241m+\u001B[39m weight_decay \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights[i])\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;66;03m#self.bias_momentum = self.bias_momentum * momentum * self.error\u001B[39;00m\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;66;03m#self.bias -= learning_rate * self.bias_momentum\u001B[39;00m\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "e5ee65cc1a9b7465",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dc675c3e9717f752"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
